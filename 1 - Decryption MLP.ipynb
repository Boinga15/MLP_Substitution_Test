{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "061b42e3-3b57-44bd-8f28-12d98cce761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "c7aec573-0a9f-4224-be81-07e4748416f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the training, validation, and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "fc3b9ef7-64ce-406b-a6f7-478410653f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encryption key is how the data set is constructed.\n",
    "originalKey = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "encryptionKey = \"zyxwvutsrqponmlkjihgfedcba\"\n",
    "\n",
    "# Parameters for the sets.\n",
    "datasetSize = 50\n",
    "trainDistribution = 0.8\n",
    "validationDistribution = 0.15\n",
    "\n",
    "minDataSize = 10\n",
    "maxDataSize = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "8ca6bc18-1170-4d86-8c6c-58427657f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the actual set.\n",
    "X_complete = []\n",
    "Y_complete = []\n",
    "\n",
    "for i in range(datasetSize):\n",
    "    datasize = random.choice(range(minDataSize, maxDataSize + 1))\n",
    "    X_value = \"\"\n",
    "    Y_value = \"\"\n",
    "\n",
    "    for j in range(datasize):\n",
    "        X_value += random.choice(originalKey)\n",
    "        Y_value += encryptionKey[originalKey.index(X_value[-1])]\n",
    "\n",
    "    X_complete.append(X_value)\n",
    "    Y_complete.append(Y_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "7e7d6a26-1072-4187-85ae-3fe49374a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the sets\n",
    "X_train = X_complete[:math.floor(datasetSize * trainDistribution)]\n",
    "Y_train = Y_complete[:math.floor(datasetSize * trainDistribution)]\n",
    "\n",
    "X_validation = X_complete[math.floor(datasetSize * trainDistribution):math.floor(datasetSize * (validationDistribution + trainDistribution))]\n",
    "Y_validation = Y_complete[math.floor(datasetSize * trainDistribution):math.floor(datasetSize * (validationDistribution + trainDistribution))]\n",
    "\n",
    "X_test = X_complete[math.floor(datasetSize * (validationDistribution + trainDistribution)):]\n",
    "Y_test = Y_complete[math.floor(datasetSize * (validationDistribution + trainDistribution)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "bd0a70cb-9e5e-4511-a1ff-a6d10b63c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the classes for the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "id": "cdcbff4c-95b9-4509-9c42-d287c684077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual input for the MLP will be a 1 x 26 matrix (a 26-dimension vector transposed). Each column is a different letter.\n",
    "\n",
    "# The first hidden layer will use a weight matrix of 26 x A, resulting in a 1 x A matrix. Each column is a different hidden node. We'll then add a \n",
    "# 1 x A bias matrix onto the layer. A is a pre-defined variable, and is the number of hidden nodes per layer.\n",
    "\n",
    "# Every hidden layer from then on will use a A x A matrix, resulting in a 1 x A matrix. Between each layer, we'll use a tanh function as our\n",
    "# non-linear function. At the end, we'll use a SoftMax function\n",
    "\n",
    "# The output layer will use a weight matrix of 32 x 26, resulting in a 1 x 26 matrix. By transposing this, we get a 26 x 1 matrix, and thus we can get a\n",
    "# valud output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "id": "9e8d568f-5431-4477-a4e0-98b603879633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixValue:\n",
    "    def __init__(self, matrix, previous = [], name = \"\"):\n",
    "        self.matrix = matrix\n",
    "        self.previous = previous\n",
    "        self.gradient = np.zeros(matrix.shape)\n",
    "        self.name = name\n",
    "\n",
    "        self.backward = lambda: None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.name == \"\":\n",
    "            return \"Value: \" + str(self.matrix) + \"\\n\\nGradient: \" + str(self.gradient)\n",
    "        else:\n",
    "            return self.name + \" -\\nValue: \" + str(self.matrix) + \"\\n\\nGradient: \" + str(self.gradient)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.name == \"\":\n",
    "            return \"Value: \" + str(self.matrix) + \"\\n\\nGradient: \" + str(self.gradient)\n",
    "        else:\n",
    "            return self.name + \" -\\nValue: \" + str(self.matrix) + \"\\n\\nGradient: \" + str(self.gradient)\n",
    "\n",
    "    def __neg__(self):\n",
    "        self.out = MatrixValue(np.negative(self.matrix),\n",
    "                               previous = [self],\n",
    "                               name = \"-\" + self.name)\n",
    "\n",
    "        def backward():\n",
    "            self.gradient += np.negative(self.out.gradient)\n",
    "\n",
    "        self.out.backward = backward\n",
    "        return self.out\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, MatrixValue):\n",
    "            self.out = MatrixValue(self.matrix + other.matrix,\n",
    "                                   previous = [self, other],\n",
    "                                   name = self.name + \" + \" + other.name)\n",
    "\n",
    "            def backward():\n",
    "                self.gradient += self.out.gradient\n",
    "                other.gradient += self.out.gradient\n",
    "\n",
    "            self.out.backward = backward\n",
    "            return self.out\n",
    "\n",
    "        else:\n",
    "            assert isinstance(other, (int, float))\n",
    "            self.out = MatrixValue(self.matrix + (np.ones(self.matrix.shape) * other),\n",
    "                                   previous = [self],\n",
    "                                   name = self.name + \" + \" + str(other))\n",
    "\n",
    "            def backward():\n",
    "                self.gradient += self.out.gradient\n",
    "\n",
    "            self.out.backward = backward\n",
    "            return self.out\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        assert isinstance(other, (MatrixValue, int, float))\n",
    "\n",
    "        result = self + (-other)\n",
    "        result.name = self.name + \" - \" + (other.name if isinstance(other, MatrixValue) else str(other))\n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, MatrixValue):\n",
    "            self.out = MatrixValue(np.matmul(self.matrix, other.matrix),\n",
    "                                   previous = [self, other],\n",
    "                                   name = self.name + \" * \" + other.name)\n",
    "\n",
    "            def backward():\n",
    "                self.gradient += np.matmul(self.out.gradient, np.transpose(other.matrix))\n",
    "                other.gradient += np.matmul(np.transpose(self.matrix), self.out.gradient)\n",
    "\n",
    "            self.out.backward = backward\n",
    "            return self.out\n",
    "\n",
    "        else:\n",
    "            assert isinstance(other, (int, float))\n",
    "            self.out = MatrixValue(self.matrix * other,\n",
    "                                   previous = [self],\n",
    "                                   name = self.name + \" * \" + str(other))\n",
    "\n",
    "            def backward():\n",
    "                self.gradient += self.out.gradient * other\n",
    "\n",
    "            self.out.backward = backward\n",
    "            return self.out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, MatrixValue):\n",
    "            self.out = MatrixValue(np.divide(self.matrix, other.matrix),\n",
    "                                   previous = [self, other],\n",
    "                                   name = self.name + \" / \" + other.name)\n",
    "\n",
    "            def backward():\n",
    "                self.gradient += np.divide(self.out.gradient, other.matrix)\n",
    "                other.gradient += np.divide(self.out.gradient, self.matrix)\n",
    "\n",
    "            self.out.backward = backward\n",
    "            return self.out\n",
    "\n",
    "        else:\n",
    "            assert isinstance(other, (int, float))\n",
    "            \n",
    "            result = self * (other**-1)\n",
    "            result.name = self.name + \" / \" + str(other)\n",
    "            return result\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "\n",
    "        self.out = MatrixValue(np.power(self.matrix, np.ones(self.matrix.shape) * other),\n",
    "                               previous = [self],\n",
    "                               name = self.name + \"^\" + str(other))\n",
    "\n",
    "        def backward():\n",
    "            result = 3 * np.power(self.matrix, np.ones(self.matrix.shape) * (other - 1))\n",
    "            self.gradient += np.multiply(self.out.gradient, result)\n",
    "\n",
    "        self.out.backward = backward\n",
    "        return self.out\n",
    "\n",
    "    def exp(self):\n",
    "        self.out = MatrixValue(np.exp(self.matrix),\n",
    "                               previous = [self],\n",
    "                               name = \"e^(\" + self.name + \")\")\n",
    "\n",
    "        def backward():\n",
    "            self.gradient += np.multiply(self.out.gradient, np.exp(self.matrix))\n",
    "\n",
    "        self.out.backward = backward\n",
    "        return self.out\n",
    "\n",
    "    def tanh(self):\n",
    "        self.out = (((self * 2).exp() - 1) / ((self * 2).exp() + 1))\n",
    "        self.out.name = \"tanh(\" + self.name + \")\"\n",
    "        self.out.previous = [self]\n",
    "\n",
    "        def backward():\n",
    "            t = MatrixValue(self.out.matrix)\n",
    "            final = ((t**2 - 1) * -1).matrix\n",
    "\n",
    "            self.gradient += np.multiply(self.out.gradient, final)\n",
    "\n",
    "        self.out.backward = backward\n",
    "        return self.out\n",
    "\n",
    "    def relu(self):\n",
    "        self.out = MatrixValue((self.matrix + np.abs(self.matrix)) / 2,\n",
    "                               previous = [self],\n",
    "                               name = \"relu(\" + self.name + \")\")\n",
    "\n",
    "        def backward():\n",
    "            initialResult = self.out.matrix\n",
    "\n",
    "            for i, row in enumerate(initialResult):\n",
    "                for j, value in enumerate(row):\n",
    "                    initialResult[i][j] = (0 if value == 0 else 1)\n",
    "\n",
    "            self.gradient += np.multiply(initialResult, self.out.gradient)\n",
    "\n",
    "        self.out.backward = backward\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def softmax(self):\n",
    "        total = np.exp(self.matrix).sum()\n",
    "\n",
    "        self.out = (self.exp() / total)\n",
    "        self.out.name = \"SoftMax(\" + self.name + \")\"\n",
    "        self.out.previous = [self]\n",
    "\n",
    "        def backward():\n",
    "            initialResult = (np.exp(self.matrix) * (total**-1)) - (np.exp(self.matrix * 2) * (total**-2))\n",
    "            self.gradient += np.multiply(self.out.gradient, initialResult)\n",
    "\n",
    "        self.out.backward = backward\n",
    "        return self.out\n",
    "\n",
    "    def abs(self):\n",
    "        self.out = MatrixValue(np.abs(self.matrix),\n",
    "                               previous = [self],\n",
    "                               name = \"abs(\" + self.name + \")\")\n",
    "\n",
    "        def backward():\n",
    "            initialResult = np.divide(self.matrix, np.abs(self.matrix))\n",
    "            self.gradient += np.multiply(self.out.gradient, initialResult)\n",
    "\n",
    "        self.out.backward = backward\n",
    "        return self.out\n",
    "    \n",
    "    def sum(self):\n",
    "        self.out = MatrixValue(np.ones((1, 1)) * self.matrix.sum(),\n",
    "                               previous = [self],\n",
    "                               name = \"sum(\" + self.name + \")\")\n",
    "\n",
    "        def backward():\n",
    "            self.gradient += np.ones(self.matrix.shape) * self.out.gradient.sum()\n",
    "\n",
    "        self.out.backward = backward\n",
    "        return self.out\n",
    "\n",
    "    def buildGradients(self):\n",
    "        self.gradient = np.ones(self.matrix.shape)\n",
    "        \n",
    "        topologicalOrder = []\n",
    "        visited = []\n",
    "\n",
    "        def buildTopology(node):\n",
    "            if not node in visited:\n",
    "                visited.append(node)\n",
    "\n",
    "                for child in node.previous:\n",
    "                    buildTopology(child)\n",
    "\n",
    "                topologicalOrder.append(node)\n",
    "\n",
    "        buildTopology(self)\n",
    "        topologicalOrder.reverse()\n",
    "\n",
    "        \"\"\"\n",
    "        for node in topologicalOrder:\n",
    "            if node != self:\n",
    "                node.gradient = np.zeros(node.matrix.shape)\n",
    "        \"\"\"\n",
    "\n",
    "        for node in topologicalOrder:\n",
    "            node.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "id": "09b0c98c-03a6-4bcb-af68-4047b2b0d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "id": "fc858092-e0f9-453b-a0e7-26fdc3f8fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(inp):\n",
    "    result = np.zeros((1, 26))\n",
    "    characterList = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    \n",
    "    result[0][characterList.index(inp)] = 1\n",
    "\n",
    "    return MatrixValue(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1339,
   "id": "de68d965-01be-4048-80d7-234ab8154a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(givenInp):\n",
    "    inp = givenInp.matrix\n",
    "    maxIndex = 0\n",
    "    maximumValue = -math.inf\n",
    "\n",
    "    for i in range(26):\n",
    "        if maximumValue < inp[0][i]:\n",
    "            maximumValue = inp[0][i]\n",
    "            maxIndex = i\n",
    "\n",
    "    return \"abcdefghijklmnopqrstuvwxyz\"[maxIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1340,
   "id": "6f8bee1c-2f89-49a8-bc7c-edb5a922ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1341,
   "id": "01368d62-5ef5-4d84-8710-84ae55aa3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateParameter(rows, columns):\n",
    "    return (np.random.rand(rows, columns) - (np.ones((rows, columns)) * 0.5)) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1342,
   "id": "83454416-73ef-49c1-be5c-f9b3f2776463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Layers and MLP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "id": "5b23045e-da38-4fd9-9478-8d5f0f03cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nodeCount, prevNodeCount, useBias = True, label = \"\"):\n",
    "        self.useBias = True\n",
    "\n",
    "        self.weights = MatrixValue(generateParameter(prevNodeCount, nodeCount))\n",
    "        self.bias = MatrixValue(generateParameter(1, nodeCount))\n",
    "        self.bias = MatrixValue(np.zeros((1, nodeCount)))\n",
    "\n",
    "        if label != \"\":\n",
    "            self.weights.name = label + \" (Weights)\"\n",
    "            self.bias.name = label + \" (Bias)\"\n",
    "\n",
    "    def calculate(self, inp):\n",
    "        weightMul = inp * self.weights\n",
    "\n",
    "        if self.useBias:\n",
    "            return weightMul + self.bias\n",
    "        else:\n",
    "            return weightMul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1344,
   "id": "c10322cc-87e8-4121-9fb0-4e3053c0902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, hiddenLayerCount, nodesPerLayer, evolutionRate = 0.01):\n",
    "        self.layers = []\n",
    "        self.parameters = []\n",
    "\n",
    "        self.evolutionRate = evolutionRate\n",
    "\n",
    "        for i in range(hiddenLayerCount + 1):\n",
    "            prevNodeCount = nodesPerLayer if i != 0 else 26\n",
    "            thisLayerCount = nodesPerLayer if i < (hiddenLayerCount) else 26\n",
    "\n",
    "            layerLabel = (\"Hidden Layer \" + str(i + 1)) if i < (hiddenLayerCount) else \"Output Layer\"\n",
    "\n",
    "            newLayer = Layer(thisLayerCount, prevNodeCount, (i < hiddenLayerCount), label = layerLabel)\n",
    "\n",
    "            self.layers.append(newLayer)\n",
    "\n",
    "            self.parameters.append(newLayer.weights)\n",
    "            self.parameters.append(newLayer.bias)\n",
    "\n",
    "    def calculate(self, inp):\n",
    "        cInput = inp\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            cInput = layer.calculate(cInput)\n",
    "\n",
    "            if i < len(self.layers) - 1:\n",
    "                cInput = cInput.tanh()\n",
    "        \n",
    "        output = (cInput).softmax()\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backPropogate(self, loss):\n",
    "        for parameter in self.parameters:\n",
    "            parameter.gradient = np.zeros(parameter.matrix.shape)\n",
    "        \n",
    "        loss.buildGradients()\n",
    "\n",
    "        for parameter in self.parameters:\n",
    "            parameter.matrix += -1 * (self.evolutionRate * parameter.gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1345,
   "id": "4375f90c-a532-465a-b46f-71e9bf8e44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1363,
   "id": "e2e31423-a9df-47f0-af15-97d34d31cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Parameters\n",
    "epochCount = 500000\n",
    "updateInterval = 1\n",
    "loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
   "id": "a5872959-23fb-4bca-bdd2-a3972afbade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodesPerHiddenLayer = 30\n",
    "hiddenLayerCount = 0\n",
    "agent = MLP(hiddenLayerCount, nodesPerHiddenLayer)\n",
    "\n",
    "agent.evolutionRate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1365,
   "id": "b17c1a08-2bf5-4b09-9a8c-eedf1d8794fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1365], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m         trueLoss \u001b[38;5;241m=\u001b[39m lossMatrix\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     15\u001b[0m         loss[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m trueLoss\u001b[38;5;241m.\u001b[39mmatrix[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)\n\u001b[1;32m---> 17\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackPropogate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrueLoss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m (j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m updateInterval:\n",
      "Cell \u001b[1;32mIn[1344], line 37\u001b[0m, in \u001b[0;36mMLP.backPropogate\u001b[1;34m(self, loss)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parameter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[0;32m     35\u001b[0m     parameter\u001b[38;5;241m.\u001b[39mgradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(parameter\u001b[38;5;241m.\u001b[39mmatrix\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuildGradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parameter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[0;32m     40\u001b[0m     parameter\u001b[38;5;241m.\u001b[39mmatrix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevolutionRate \u001b[38;5;241m*\u001b[39m parameter\u001b[38;5;241m.\u001b[39mgradient)\n",
      "Cell \u001b[1;32mIn[1234], line 229\u001b[0m, in \u001b[0;36mMatrixValue.buildGradients\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03mfor node in topologicalOrder:\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    if node != self:\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m        node.gradient = np.zeros(node.matrix.shape)\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m topologicalOrder:\n\u001b[1;32m--> 229\u001b[0m     node\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(epochCount):\n",
    "    loss.append(0)\n",
    "    for j in range(len(X_train)):\n",
    "        X = X_train[j]\n",
    "        Y = Y_train[j]\n",
    "        \n",
    "        for k, character in enumerate(X):\n",
    "            agentResult = agent.calculate(encoder(character))\n",
    "            expectedResult = encoder(Y[k])\n",
    "\n",
    "            lossMatrix = expectedResult - agentResult\n",
    "            trueLoss = lossMatrix.abs().sum()\n",
    "\n",
    "            loss[-1] += trueLoss.matrix[0][0] / len(X)\n",
    "\n",
    "            agent.backPropogate(trueLoss)\n",
    "\n",
    "    loss[-1] /= (j + 1)\n",
    "    \n",
    "    if (epoch + 1) % updateInterval:\n",
    "        print(\"Epoch #\" + str(epoch + 1) + \": Loss - \" + loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "id": "a9c74c23-ac27-47b0-88f8-d28369a2e91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input value: krkoryhjrwrqqiyjiyurv\n",
      "\n",
      "Expected result: piplibsqidijjrbqrbfie\n",
      "Obtained result: piplibsqidijjrbqrbfie\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "randomIndex = random.choice(range(len(X_validation)))\n",
    "\n",
    "X_random = X_validation[randomIndex]\n",
    "Y_random = Y_validation[randomIndex]\n",
    "\n",
    "agentResult = \"\"\n",
    "\n",
    "for character in X_random:\n",
    "    agentResult += decoder(agent.calculate(encoder(character)))\n",
    "\n",
    "print(\"Input value: \" + X_random)\n",
    "print(\"\\nExpected result: \" + Y_random)\n",
    "print(\"Obtained result: \" + agentResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1370,
   "id": "9bbeb143-870c-4d86-b30f-36362a97114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 5.5969460333487575e-05\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "totalLoss = 0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X = X_test[i]\n",
    "    Y = Y_test[i]\n",
    "\n",
    "    dataLoss = 0\n",
    "\n",
    "    for j, character in enumerate(X):\n",
    "        agentResult = agent.calculate(encoder(character))\n",
    "        expectedResult = encoder(Y[j])\n",
    "\n",
    "        lossMatrix = expectedResult - agentResult\n",
    "        trueLoss = lossMatrix.abs().sum()\n",
    "\n",
    "        dataLoss += trueLoss.matrix[0][0] / len(X)\n",
    "\n",
    "    totalLoss += dataLoss / len(X_test)\n",
    "\n",
    "print(\"Average Loss: \" + str(totalLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f114f30-65c4-4e2c-bf93-c4dfe518e73a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
